import sqlite3
import pandas as pd # pyright: ignore[reportMissingModuleSource]
import os
import gc
from datetime import datetime
import subprocess

# ========== å‚æ•°é…ç½® ==========
DB_PATH = "data.sqlite"
TABLE_NAME = "records"
DATA_FOLDER = "data"
CHUNK_SIZE = 10000  # æ¯æ‰¹è¯»å–è¡Œæ•°

def init_database():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute(f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            seed INTEGER PRIMARY KEY,
            count INTEGER
        );
    """)
    conn.commit()
    return conn

def remove_problematic_lines(csv_path):
    """
    é€è¡Œè¯»å– CSV æ–‡ä»¶ï¼Œè·³è¿‡å¼•å‘ _csv.Error çš„è¡Œã€‚
    """
    temp_file = csv_path + ".tmp"
    error_lines = []

    try:
        with open(csv_path, 'r', encoding='utf-8') as fin, \
             open(temp_file, 'w', encoding='utf-8', newline='') as fout:
            reader = csv.reader(fin)
            writer = csv.writer(fout)
            line_num = 0
            while True:
                line_num += 1
                try:
                    row = next(reader)
                    writer.writerow(row)
                except StopIteration:
                    break
                except Exception as e:
                    if "_csv.Error" in str(type(e)):
                        error_lines.append(line_num)
                    else:
                        raise  # éCSVé”™è¯¯åˆ™æŠ›å‡º
    except Exception as e:
        log(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        if os.path.exists(temp_file):
            os.remove(temp_file)
        return False

    if error_lines:
        log(f"ğŸ—‘ è‡ªåŠ¨åˆ é™¤ä»¥ä¸‹å‡ºé”™è¡Œå·: {error_lines}")
        os.replace(temp_file, csv_path)
        return True
    else:
        os.remove(temp_file)
        return False

# ========== æ—¥å¿—è¾“å‡º ==========
def log(msg):
    ts = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{ts} {msg}")

# ========== è·å–å·²å­˜åœ¨çš„ seed ==========
def get_existing_seeds(conn):
    try:
        return set(pd.read_sql(f"SELECT seed FROM {TABLE_NAME}", conn)["seed"].values)
    except:
        return set()

# ========== å•ä¸ª CSV å†™å…¥ ==========
def insert_csv_to_db(csv_path, conn, chunk_size):
    log(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{csv_path}")

    # è®¾ç½®æœ€å¤§å­—æ®µå¤§å°
    try:
        csv.field_size_limit(min(2147483647, sys.maxsize))
    except OverflowError:
        pass

    error_count = 0
    success_count = 0

    try:
        csv_reader = pd.read_csv(
            csv_path,
            chunksize=chunk_size,
            engine='python',
            on_bad_lines='skip'
        )
    except Exception as e:
        log(f"âš ï¸ åˆæ¬¡è¯»å–å¤±è´¥: {e}")
        if remove_problematic_lines(csv_path):
            log("ğŸ” å·²å°è¯•æ¸…ç†é—®é¢˜è¡Œï¼Œé‡æ–°åŠ è½½...")
            try:
                csv_reader = pd.read_csv(
                    csv_path,
                    chunksize=chunk_size,
                    engine='python',
                    on_bad_lines='skip'
                )
            except Exception as retry_e:
                log(f"âŒ äºŒæ¬¡åŠ è½½ä»å¤±è´¥: {retry_e}")
                return
        else:
            log("âŒ æœªå‘ç°æ˜æ˜¾é—®é¢˜è¡Œï¼Œæ— æ³•è‡ªåŠ¨ä¿®å¤ã€‚")
            return

    for i, chunk in enumerate(csv_reader):
        try:
            insert_unique_rows(chunk, conn, i, len(chunk))
            success_count += 1
        except Exception as e:
            error_count += 1
            log(f"âš ï¸ ç¬¬ {i + 1} æ‰¹æ•°æ®å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡: {e}")
        finally:
            del chunk
            gc.collect()

    log(f"âœ… å¤„ç†å®Œæˆï¼šæˆåŠŸ {success_count} æ‰¹ï¼Œå¤±è´¥ {error_count} æ‰¹")

def insert_unique_rows(df, conn, i=0, len_chunk=0):
    try:
        df.columns = [c.lower() for c in df.columns]
        if not {'seed', 'count'}.issubset(df.columns):
            log("âš ï¸ CSVç¼ºå°‘å¿…è¦åˆ— seed å’Œ countï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            return

        df = df[['seed', 'count']].copy()
        df.dropna(inplace=True)

        # ç±»å‹è½¬æ¢ + å»é‡
        df['seed'] = df['seed'].astype(int)
        df['count'] = df['count'].astype(int)
        df.drop_duplicates(subset=['seed'], keep='first', inplace=True)

        if df.empty:
            log("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¡Œ")
            return

        # ä½¿ç”¨ INSERT OR IGNORE é¿å…é‡å¤æ’å…¥
        insert_sql = f"INSERT OR IGNORE INTO {TABLE_NAME} (seed, count) VALUES (?, ?)"
        data_tuples = list(df.itertuples(index=False, name=None))

        # ä½¿ç”¨äº‹åŠ¡æé«˜æ•ˆç‡
        conn.execute("BEGIN")
        conn.executemany(insert_sql, data_tuples)
        conn.execute("COMMIT")

        log(f"âœ… æˆåŠŸæ’å…¥ {len(data_tuples)} æ¡è®°å½• - å¤„ç†ç¬¬ {i + 1} æ‰¹æ•°æ®ï¼Œå…± {len_chunk} è¡Œ")

    except Exception as e:
        conn.execute("ROLLBACK")
        log(f"âš ï¸ æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡: {e}")

# ========== å¤„ç† backup/ ç›®å½•ä¸‹æ‰€æœ‰ CSV ==========
def init_db_from_backup():
    conn = init_database()
    for fname in os.listdir(DATA_FOLDER):
        if fname.endswith(".csv"):
            fpath = os.path.join(DATA_FOLDER, fname)
            insert_csv_to_db(fpath, conn, CHUNK_SIZE)
    conn.close()
    log("âœ… æ‰€æœ‰ CSV æ–‡ä»¶å¯¼å…¥å®Œæˆ")

# ========== æŸ¥è¯¢æœ€å¤§ count ==========
def get_max_count_info():
    conn = sqlite3.connect(DB_PATH)
    row = conn.execute(f"""
        SELECT seed, count FROM {TABLE_NAME}
        ORDER BY count DESC LIMIT 1;
    """).fetchone()
    conn.close()
    log(f"ğŸ“Š æœ€å¤§ count: {row}")
    return row

# ========== æŸ¥è¯¢æ•°æ®åŒºé—´ ==========
def get_seed_range():
    conn = sqlite3.connect(DB_PATH)
    row = conn.execute(f"SELECT MIN(seed), MAX(seed) FROM {TABLE_NAME};").fetchone()
    conn.close()
    min_seed, max_seed = row[0], row[1]
    log(f"ğŸ“Š æ•°æ®åŒºé—´: {min_seed} - {max_seed}")
    print()
    return min_seed, max_seed

def find_gap_seeds(start_seed=0):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.execute(f"SELECT seed FROM {TABLE_NAME} WHERE seed > ? ORDER BY seed", (start_seed,))
    
    gaps_file = "gaps.txt"
    large_gaps = []
    is_gap = False
    
    # ç¼“å†²åŒºå¤§å°ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨
    BUFFER_SIZE = CHUNK_SIZE
    gap_buffer = []
    
    def flush_buffer(buffer, file_handle):
        """å°†ç¼“å†²åŒºå†…å®¹å†™å…¥æ–‡ä»¶"""
        if buffer:
            file_handle.write('\n'.join(map(str, buffer)) + '\n')
            buffer.clear()
    
    # æ‰“å¼€æ–‡ä»¶å‡†å¤‡å†™å…¥å°é—´éš™
    with open(gaps_file, "w") as f:
        # é€è¡Œè¯»å–ç§å­ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
        prev_seed = None
        for row in cursor:
            current_seed = row[0]
            if prev_seed is not None:
                gap = current_seed - prev_seed
                if gap > 1:
                    is_gap = True
                    start_gap = prev_seed + 1
                    end_gap = current_seed - 1
                    gap_count = end_gap - start_gap + 1
                    
                    if gap_count < 50000:
                        # å¯¹äºå°é—´éš™ï¼Œä½¿ç”¨rangeç”Ÿæˆå¹¶ç¼“å†²å†™å…¥
                        gap_seeds = list(range(start_gap, end_gap + 1))
                        if len(gap_buffer) + len(gap_seeds) > BUFFER_SIZE:
                            flush_buffer(gap_buffer, f)
                        gap_buffer.extend(gap_seeds)
                        # å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œç«‹å³å†™å…¥
                        if len(gap_buffer) >= BUFFER_SIZE:
                            flush_buffer(gap_buffer, f)
                    else:
                        # è¾“å‡ºå¤§é—´éš™å‘½ä»¤
                        print()
                        print(f"slime_finder --start-seed {start_gap} --threads 1 --iterations {gap_count} --output results.csv")
                        large_gaps.append(f"{start_gap}ï½{end_gap}")
            prev_seed = current_seed
        
        # å†™å…¥å‰©ä½™çš„ç¼“å†²åŒºå†…å®¹
        flush_buffer(gap_buffer, f)
    
    conn.close()
    
    # å¤„ç†å°é—´éš™
    if is_gap and os.path.exists(gaps_file) and os.path.getsize(gaps_file) > 0:
        # è®¡ç®—æ€»é—´éš™ç§å­æ•°ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦ç²¾ç¡®ç»Ÿè®¡ï¼‰
        total_gaps = sum(1 for line in open(gaps_file))
        log(f"â–¶ï¸ å¼€å§‹è°ƒç”¨ slime_finder è¿›è¡Œè¡¥å…¨ï¼Œå…±æœ‰ {total_gaps} ä¸ªé—´æ–­ç§å­")
        subprocess.run(["slime_finder", "--seed-file", gaps_file, "--threads", "15", "--output", "results.csv"])
        log("âœ… slime_finder è¿è¡Œå®Œæˆ")
        data_write()
        os.remove(gaps_file)
        log("âœ… æ•°æ®è¡¥å…¨å®Œæˆ")
    elif is_gap:
        log("âœ… æ•°æ®è¿ç»­ï¼ˆæ— å°é—´éš™éœ€è¦å¤„ç†ï¼‰")
    else:
        log("âœ… æ•°æ®è¿ç»­")

def get_record_count():
    """
    æŸ¥è¯¢å¹¶è¾“å‡º SQLite æ•°æ®åº“ä¸­ records è¡¨çš„æ€»è¡Œæ•°ã€‚
    """
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(f"SELECT COUNT(*) FROM {TABLE_NAME};")
    count = cursor.fetchone()[0]
    conn.close()
    log(f"ğŸ“¦ æ•°æ®åº“å½“å‰å…± {count} æ¡è®°å½•")
    return count

# ========== è·å–æ‰€æœ‰ä¸åŒcountçš„æ•°é‡ ==========
def get_distinct_counts():
    """
    è·å–æ•°æ®åº“ä¸­æ‰€æœ‰ä¸åŒçš„countå€¼åŠå…¶å‡ºç°æ¬¡æ•°
    
    è¿”å›:
        dict: é”®ä¸ºcountå€¼ï¼Œå€¼ä¸ºå¯¹åº”çš„å‡ºç°æ¬¡æ•°
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        # æŸ¥è¯¢ä¸åŒcountå€¼çš„æ•°é‡
        df = pd.read_sql(f"""
            SELECT count, COUNT(*) as frequency 
            FROM {TABLE_NAME}
            GROUP BY count
            ORDER BY count DESC;
        """, conn)
        log("ğŸ“Š ä¸åŒcountå€¼çš„ç»Ÿè®¡ä¿¡æ¯:")
        print(df.to_string(index=False))
        # è½¬æ¢ä¸ºå­—å…¸ï¼š{count: frequency}
        return dict(zip(df['count'], df['frequency']))
    finally:
        conn.close()

# ========== æ‰“å°å‰ä¸‰å¤§countçš„æ‰€æœ‰æ•°æ® ==========
def print_top3_counts_data(top_count):
    """
    æŸ¥è¯¢å¹¶æ‰“å°å‰ä¸‰å¤§countå€¼çš„æ‰€æœ‰ç§å­æ•°æ®
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        # é¦–å…ˆè·å–å‰ä¸‰å¤§ä¸åŒçš„countå€¼
        top_counts = conn.execute(f"""
            SELECT DISTINCT count FROM {TABLE_NAME}
            ORDER BY count DESC
            LIMIT {top_count};
        """).fetchall()
        
        if not top_counts:
            log("â„¹ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®")
            return
            
        top_counts = [count[0] for count in top_counts]
        log(f"ğŸ” å‰ä¸‰å¤§countå€¼: {', '.join(map(str, top_counts))}")
        
        # æŸ¥è¯¢è¿™äº›countå€¼çš„æ‰€æœ‰æ•°æ®
        for count in top_counts:
            log(f"\nğŸ“‹ count = {count} çš„æ‰€æœ‰è®°å½•:")
            df = pd.read_sql(f"""
                SELECT seed, count 
                FROM {TABLE_NAME}
                WHERE count = {count}
                ORDER BY seed;
            """, conn)
            
            if df.empty:
                print(f"æ²¡æœ‰count={count}çš„è®°å½•")
            else:
                print(f"å…±{len(df)}æ¡è®°å½•:")
                print(df.to_string(index=False))
                
    finally:
        conn.close()


def data_write():
    filepath="results.csv"
    if os.path.exists(filepath):
        conn = init_database()
        insert_csv_to_db(filepath, conn, CHUNK_SIZE)
        conn.close()
        os.remove(filepath)
        print("æ–‡ä»¶åˆ é™¤æˆåŠŸ")
    print("æ•°æ®å†™å…¥å®Œæˆ")
# ========== ä¸»å‡½æ•°å…¥å£ ==========
def main():
    log("ğŸš€ ç¨‹åºå¼€å§‹")
    # init_db_from_backup()

    data_write()
    get_seed_range()
    get_record_count()  
    get_max_count_info()
    find_gap_seeds(0)
    print(get_distinct_counts())
    print_top3_counts_data(2)

    # print("slime_finder --seed-file gaps.txt --threads 15 --output results.csv")
    log("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()