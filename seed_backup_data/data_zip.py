import sqlite3
import os
import json

# === é…ç½®å‚æ•° ===
DB_PATH = "data.sqlite"
TABLE_NAME = "records"
BACKUP_DIR = "backup"
CHUNK_SIZE = 100_000
OFFSET = 37
BITS_PER_COUNT = 5
VOLUME_SIZE = 1_000_000_000  # æ¯å·æœ€å¤§è®°å½•æ•°
META_PATH = os.path.join(BACKUP_DIR, "metadata.json")

# === åˆå§‹åŒ–ç›®å½• ===
os.makedirs(BACKUP_DIR, exist_ok=True)

# === å…ƒæ•°æ®å¤„ç† ===
def load_metadata():
    if os.path.exists(META_PATH):
        with open(META_PATH, "r") as f:
            return json.load(f)
    else:
        return {
            "start_seed": 0,
            "end_seed": -1,
            "count_histogram": {},
            "offset": OFFSET,
            "current_volume": 0
        }

def save_metadata(meta):
    with open(META_PATH, "w") as f:
        json.dump(meta, f)

# === å·ç¼–å·ä¸è·¯å¾„ ===
def get_volume_number(seed):
    return seed // VOLUME_SIZE

def get_volume_path(volume):
    return os.path.join(BACKUP_DIR, f"archive_{volume:03d}.bin")

# === è¿ç»­æ€§éªŒè¯ ===
def is_continuous(rows, expected_start):
    for i, (seed, _) in enumerate(rows):
        if seed != expected_start + i:
            print(f"âŒ éè¿ç»­ seed: æœŸæœ› {expected_start + i}, å®é™… {seed}")
            return False
    return True

# === ä½çº§å‹ç¼©å‡½æ•° ===
def compress_block_to_bytes(rows, offset):
    bitstream = 0
    bit_length = 0
    byte_array = bytearray()

    for seed, count in rows:
        val = count - offset
        if not (0 <= val < (1 << BITS_PER_COUNT)):
            raise ValueError(f"count {count} è¶…å‡ºå‹ç¼©èŒƒå›´")
        bitstream = (bitstream << BITS_PER_COUNT) | val
        bit_length += BITS_PER_COUNT

        while bit_length >= 8:
            byte = (bitstream >> (bit_length - 8)) & 0xFF
            byte_array.append(byte)
            bit_length -= 8

    if bit_length != 0:
        print("âš ï¸ éå¯¹é½ä½æµï¼Œä¸­æ­¢å¤„ç†")
        return None

    return bytes(byte_array)

# === ä¸»å½’æ¡£é€»è¾‘ ===
def archive_data():
    conn = sqlite3.connect(DB_PATH)
    meta = load_metadata()
    start = meta["end_seed"] + 1
    total_written = 0
    current_volume = get_volume_number(start)
    bin_path = get_volume_path(current_volume)
    bin_file = open(bin_path, "ab")

    try:
        while True:
            cursor = conn.execute(f"""
                SELECT seed, count FROM {TABLE_NAME}
                WHERE seed >= ? AND seed < ?
                ORDER BY seed ASC
            """, (start, start + CHUNK_SIZE))
            rows = cursor.fetchall()

            if len(rows) < CHUNK_SIZE:
                print(f"âš ï¸ å½“å‰å—ä»…æœ‰ {len(rows)} æ¡è®°å½•ï¼Œæœªæ»¡ {CHUNK_SIZE}ï¼Œä¸­æ­¢å¤„ç†")
                break

            if not is_continuous(rows, start):
                print("âš ï¸ å½“å‰å—æ•°æ®ä¸è¿ç»­ï¼Œä¸­æ­¢å¤„ç†")
                break

            compressed = compress_block_to_bytes(rows, OFFSET)
            if compressed is None or len(compressed) != CHUNK_SIZE * BITS_PER_COUNT // 8:
                print("âŒ å‹ç¼©å¤±è´¥æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œä¸­æ­¢å¤„ç†")
                break

            bin_file.write(compressed)

            for _, count in rows:
                meta["count_histogram"][str(count)] = meta["count_histogram"].get(str(count), 0) + 1

            meta["end_seed"] = rows[-1][0]
            new_volume = get_volume_number(meta["end_seed"] + 1)
            if new_volume != current_volume:
                bin_file.close()
                current_volume = new_volume
                bin_path = get_volume_path(current_volume)
                bin_file = open(bin_path, "ab")
                print(f"ğŸ“ å·åˆ‡æ¢è‡³ archive_{current_volume:03d}.bin")

            meta["current_volume"] = current_volume
            save_metadata(meta)
            print(f"âœ… å†™å…¥ seed {start} åˆ° {meta['end_seed']}")
            start = meta["end_seed"] + 1
            total_written += CHUNK_SIZE

    finally:
        bin_file.close()
        conn.close()

    print(f"ğŸ‰ å†·å½’æ¡£å®Œæˆï¼Œå…±å†™å…¥ {total_written} æ¡è®°å½•")

if __name__ == "__main__":
    archive_data()